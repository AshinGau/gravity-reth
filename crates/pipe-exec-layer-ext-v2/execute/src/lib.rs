//! Pipeline execution layer extension
#[macro_use]
mod channel;
mod metrics;
pub mod mint_precompile;
pub mod onchain_config;
use alloy_sol_types::SolEvent;

use channel::Channel;
use gravity_api_types::{
    config_storage::{BlockNumber, ConfigStorage, OnChainConfig, OnChainConfigResType},
    events::contract_event::GravityEvent,
    ExtraDataType,
};
use metrics::PipeExecLayerMetrics;

use alloy_consensus::{
    constants::EMPTY_WITHDRAWALS, BlockHeader, Header, Transaction, EMPTY_OMMER_ROOT_HASH,
};
use alloy_eips::{eip4895::Withdrawals, merge::BEACON_NONCE, BlockNumberOrTag};
use alloy_primitives::{
    map::{HashMap, HashSet},
    Address, Bytes, TxHash, B256, U256,
};
use alloy_rpc_types_eth::TransactionRequest;
use gravity_primitives::get_gravity_config;
use grevm::ParallelState;
use rayon::iter::{IntoParallelIterator, ParallelIterator};
use reth_chain_state::{ExecutedBlockWithTrieUpdates, ExecutedTrieUpdates};
use reth_chainspec::{ChainSpec, EthereumHardforks};
use reth_ethereum_primitives::{Block, BlockBody, Receipt, TransactionSigned};
use reth_evm::{ConfigureEvm, Evm, NextBlockEnvAttributes, ParallelDatabase};
use reth_evm_ethereum::EthEvmConfig;
use reth_execution_types::{BlockExecutionOutput, ExecutionOutcome};
use reth_pipe_exec_layer_event_bus::{
    MakeCanonicalEvent, PipeExecLayerEvent, PipeExecLayerEventBus, WaitForPersistenceEvent,
    PIPE_EXEC_LAYER_EVENT_BUS,
};
use reth_primitives::EthPrimitives;
use reth_primitives_traits::{
    proofs::{self},
    Block as _, RecoveredBlock,
};
use reth_provider::{OriginalValuesKnown, PersistBlockCache, PERSIST_BLOCK_CACHE};
use reth_rpc_eth_api::RpcTypes;
use revm::{
    database::{states::bundle_state::BundleRetention, State},
    state::AccountInfo,
    Database, DatabaseCommit,
};
use std::{
    collections::BTreeMap,
    sync::{
        atomic::{AtomicU64, Ordering},
        Arc,
    },
    time::{Duration, Instant},
};

use gravity_storage::GravityStorage;
use onchain_config::OnchainConfigFetcher;
use reth_rpc_eth_api::helpers::EthCall;
use reth_trie::{HashedPostState, KeccakKeyHasher};
use tokio::sync::{
    mpsc::{UnboundedReceiver, UnboundedSender},
    oneshot, Mutex,
};
use tracing::*;

use crate::{
    mint_precompile::create_mint_token_precompile,
    onchain_config::{
        construct_metadata_txn, construct_validator_txn_from_extra_data,
        dkg::{convert_dkg_start_event_to_api, DKGStartEvent},
        transact_system_txn,
        types::DataRecorded,
        SystemTxnResult, NATIVE_MINT_PRECOMPILE_ADDR, SYSTEM_CALLER,
    },
};

/// Metadata about an executed block
#[derive(Debug, Clone, Copy)]
pub struct ExecutedBlockMeta {
    /// Which ordered block is used to execute the block
    pub block_id: B256,
    /// Block hash of the executed block
    pub block_hash: B256,
    /// Block number of the executed block
    pub block_number: u64,
}

/// An ordered block received from Coordinator for execution
#[derive(Debug)]
pub struct OrderedBlock {
    /// Epoch of the block
    pub epoch: u64,
    /// `BlockId` of the parent block generated by Gravity SDK
    pub parent_id: B256,
    /// `BlockId` of the block generated by Gravity SDK
    pub id: B256,
    /// Block number of the block
    pub number: u64,
    /// Timestamp of the block in microseconds
    pub timestamp_us: u64,
    /// Coinbase address of the block
    pub coinbase: Address,
    /// Block header mix hash, used as prevRandao in the block
    pub prev_randao: B256,
    /// Withdrawals in the block, if any
    /// See <https://github.com/ethereum/execution-apis/blob/6709c2a795b707202e93c4f2867fa0bf2640a84f/src/engine/shanghai.md#executionpayloadv2>
    pub withdrawals: Withdrawals,
    /// Ordered transactions in the block
    pub transactions: Vec<TransactionSigned>,
    /// Senders of the transactions in the block
    pub senders: Vec<Address>,
    /// The proposer index in the active validator set (None for NIL blocks)
    pub proposer_index: Option<u64>,
    /// Validator-related transactions (JWK updates and DKG transcripts) sent by system caller
    pub extra_data: Vec<ExtraDataType>,
    /// Randomness for the block, generated by DKG
    pub randomness: U256,
}

enum ReceivedBlock {
    /// Block received from Coordinator
    #[allow(clippy::large_enum_variant)]
    OrderedBlock(OrderedBlock),
    /// History block to be processed. Only used for testing.
    HistoryBlock(Box<RecoveredBlock<Block>>),
}

impl ReceivedBlock {
    fn id(&self) -> B256 {
        match self {
            Self::OrderedBlock(block) => block.id,
            Self::HistoryBlock(block) => block.hash(),
        }
    }

    fn parent_id(&self) -> B256 {
        match self {
            Self::OrderedBlock(block) => block.parent_id,
            Self::HistoryBlock(block) => block.parent_hash(),
        }
    }

    fn number(&self) -> u64 {
        match self {
            Self::OrderedBlock(block) => block.number,
            Self::HistoryBlock(block) => block.number(),
        }
    }

    fn epoch(&self) -> u64 {
        match self {
            Self::OrderedBlock(block) => block.epoch,
            Self::HistoryBlock(_) => 0,
        }
    }
}

/// Events emitted by the pipeline execution layer
#[derive(Debug)]
pub struct ExecutionArgs {
    /// The latest block number and its corresponding block id.
    pub block_number_to_block_id: BTreeMap<u64, B256>,
}

/// Owned by EL
#[derive(Debug)]
struct PipeExecService<Storage: GravityStorage> {
    /// Immutable part of the state
    core: Arc<Core<Storage>>,
    /// Receive ordered block from Coordinator
    ordered_block_rx: UnboundedReceiver<ReceivedBlock>,
    /// Receive the execution init args from `GravitySDK`
    execution_args_rx: oneshot::Receiver<ExecutionArgs>,
}

/// Information about a transaction in the executed block
#[derive(Debug, Clone)]
pub struct TxInfo {
    /// Transaction hash
    pub tx_hash: TxHash,
    /// Sender address of the transaction
    pub sender: Address,
    /// Nonce of the transaction
    pub nonce: u64,
    /// Whether the transaction is discarded due to validation failure
    pub is_discarded: bool,
}

/// Result of the executed block
#[derive(Debug, Clone)]
pub struct ExecutionResult {
    /// Block id of the executed block
    /// This is the block id generated by Gravity SDK
    pub block_id: B256,
    /// Block number of the executed block
    pub block_number: u64,
    /// Block hash of the executed block
    /// This is the block hash generated by EL after execution and merklization
    pub block_hash: B256,
    /// Information about the transactions in the executed block
    pub txs_info: Vec<TxInfo>,
    /// Gravity events emitted by the executed block
    pub gravity_events: Vec<GravityEvent>,
}

#[derive(Debug, Clone)]
struct ExecuteBlockContext {
    parent_header: Header,
    prev_start_execute_time: Instant,
    epoch: u64,
}

#[derive(Debug)]
struct Core<Storage: GravityStorage> {
    /// Send executed block hash to Coordinator
    execution_result_tx: UnboundedSender<ExecutionResult>,
    /// Receive verified block hash from Coordinator
    verified_block_hash_rx: Arc<Channel<B256 /* block id */, Option<B256> /* block hash */>>,
    storage: Arc<Storage>,
    evm_config: EthEvmConfig,
    chain_spec: Arc<ChainSpec>,
    event_tx: std::sync::mpsc::Sender<PipeExecLayerEvent<EthPrimitives>>,
    execute_block_barrier: Channel<(u64, u64) /* epoch, block number */, ExecuteBlockContext>,
    merklize_barrier: Channel<u64 /* block number */, ()>,
    seal_barrier: Channel<u64 /* block number */, B256 /* block hash */>,
    make_canonical_barrier: Channel<u64 /* block number */, Instant>,
    discard_txs_tx: UnboundedSender<Vec<TxHash>>,
    cache: PersistBlockCache,
    epoch: AtomicU64,
    execute_height: AtomicU64,
    metrics: PipeExecLayerMetrics,
}

impl<Storage: GravityStorage> PipeExecService<Storage> {
    async fn run(mut self) {
        self.core.init_storage(self.execution_args_rx.await.unwrap());
        loop {
            let start_time = Instant::now();
            let block = match self.ordered_block_rx.recv().await {
                Some(block) => block,
                None => {
                    self.core.execute_block_barrier.close();
                    self.core.merklize_barrier.close();
                    self.core.make_canonical_barrier.close();
                    return;
                }
            };
            let elapsed = start_time.elapsed();
            self.core.metrics.recv_block_time_diff.record(elapsed);
            info!(target: "PipeExecService.run",
                id=?block.id(),
                parent_id=?block.parent_id(),
                number=?block.number(),
                epoch=?block.epoch(),
                elapsed=?elapsed,
                "new ordered block"
            );

            let core = self.core.clone();
            tokio::spawn(async move {
                let start_time = Instant::now();
                core.process(block).await;
                core.metrics.process_block_duration.record(start_time.elapsed());
            });
        }
    }
}

#[derive(Debug)]
struct ExecuteOrderedBlockResult {
    /// Block without roots and block hash
    block: Block,
    senders: Vec<Address>,
    execution_output: BlockExecutionOutput<Receipt>,
    txs_info: Vec<TxInfo>,
    gravity_events: Vec<GravityEvent>,
    epoch: u64,
}

/// Result of system transaction execution
///
/// System transactions may trigger an epoch change, which requires early return.
/// This enum represents both outcomes.
enum SystemTxnExecutionOutcome {
    /// Normal execution completed, continue with block processing
    Continue {
        metadata_result: SystemTxnResult,
        accumulated_state_changes: revm::state::EvmState,
        validator_results: Vec<SystemTxnResult>,
    },
    /// Epoch changed, return early with the result
    EpochChanged(ExecuteOrderedBlockResult),
}

#[cfg(debug_assertions)]
fn validate_execution_output(
    block: &Block,
    execution_output: &BlockExecutionOutput<Receipt>,
) -> Result<(), String> {
    if block.gas_limit() < block.gas_used() {
        return Err(format!("gas_limit({}) < gas_used({})", block.gas_limit(), block.gas_used()));
    }
    let expected_gas_used =
        execution_output.receipts.last().map(|r| r.cumulative_gas_used).unwrap_or(0);
    if block.gas_used() != expected_gas_used {
        return Err(format!(
            "block gas_used({}) != last receipt cumulative_gas_used({})",
            block.gas_used(),
            expected_gas_used
        ));
    }
    if execution_output.gas_used != block.gas_used {
        return Err(format!(
            "execution_output.gas_used({}) != block.gas_used({})",
            execution_output.gas_used, block.gas_used
        ));
    }
    let now_secs =
        std::time::SystemTime::now().duration_since(std::time::UNIX_EPOCH).unwrap().as_secs();
    if block.timestamp() > now_secs * 2 {
        return Err(format!(
            "block timestamp({}) is not in seconds, likely in milliseconds or microseconds",
            block.timestamp()
        ));
    }
    Ok(())
}

impl<Storage: GravityStorage> Core<Storage> {
    fn epoch(&self) -> u64 {
        self.epoch.load(Ordering::Acquire)
    }

    fn execute_height(&self) -> u64 {
        self.execute_height.load(Ordering::Acquire)
    }

    async fn process(&self, block: ReceivedBlock) {
        // Wait untile there's no large gap between cache and db
        let block_number = block.number();
        let block_id = block.id();
        let (randomness, block_epoch) = if let ReceivedBlock::OrderedBlock(ordered_block) = &block {
            (ordered_block.randomness, ordered_block.epoch)
        } else {
            (U256::ZERO, self.epoch())
        };
        self.metrics.start_process_block_number.set(block_number as f64);

        // Retrieve the parent block header to generate the necessary configs for
        // executing the current block
        let ExecuteBlockContext { parent_header, prev_start_execute_time, epoch } = loop {
            info!(
                "Wait execute_block_barrier {} => ({}, {})",
                block_number,
                block_epoch,
                block_number - 1
            );
            match self
                .execute_block_barrier
                .wait_timeout((block_epoch, block_number - 1), Duration::from_secs(2))
                .await
            {
                Some(parent) => break parent,
                // Make sure the ordered blocks are idempotent
                None => {
                    if block_epoch < self.epoch() || block_number <= self.execute_height() {
                        warn!(target: "PipeExecService.process",
                            block_number=?block_number,
                            block_id=?block_id,
                            block_epoch=?block_epoch,
                            current_epoch=?self.epoch(),
                            execute_height=?self.execute_height(),
                            "epoch or execute height mismatch"
                        );
                        return;
                    } else {
                        warn!(target: "PipeExecService.process",
                            block_number=?block_number,
                            block_id=?block_id,
                            block_epoch=?block_epoch,
                            "timeout(2s) wait for execute_block_barrier"
                        );
                    }
                }
            }
        };
        if let ReceivedBlock::OrderedBlock(ordered_block) = &block {
            assert!(ordered_block.epoch == epoch);
        }
        self.storage.insert_block_id(block_number, block_id);

        // Wait for persist gap with a reasonable timeout (2 seconds)
        self.cache.wait_persist_gap(Some(2000));
        let start_time = Instant::now();
        let ExecuteOrderedBlockResult {
            mut block,
            senders,
            execution_output,
            txs_info,
            gravity_events,
            epoch,
        } = match block {
            ReceivedBlock::OrderedBlock(ordered_block) => {
                self.execute_ordered_block(ordered_block, &parent_header)
            }
            ReceivedBlock::HistoryBlock(recovered_block) => {
                self.execute_history_block(*recovered_block)
            }
        };

        #[cfg(debug_assertions)]
        validate_execution_output(&block, &execution_output).unwrap_or_else(|e| {
            panic!("validate_execution_output failed. error: {e:?}\n{:?}", block.header());
        });

        let write_start = Instant::now();
        self.cache.write_state_changes(
            block_number,
            OriginalValuesKnown::No,
            &execution_output.state.state,
            &execution_output.state.contracts,
        );
        let hashed_state =
            HashedPostState::from_bundle_state::<KeccakKeyHasher>(&execution_output.state.state);
        self.metrics.cache_account_state.record(write_start.elapsed());
        let elapsed = start_time.elapsed();
        info!(target: "PipeExecService.process",
            block_number=?block_number,
            block_id=?block_id,
            gas_used=execution_output.gas_used,
            elapsed=?elapsed,
            epoch=?epoch,
            "block executed"
        );
        self.metrics.execute_duration.record(elapsed);
        self.metrics.start_execute_time_diff.record(start_time - prev_start_execute_time);

        if epoch > block_epoch {
            info!(target: "PipeExecService.process",
                block_number=?block_number,
                block_id=?block_id,
                prev_epoch=?block_epoch,
                new_epoch=?epoch,
                "new epoch"
            );
            assert_eq!(self.epoch.fetch_max(epoch, Ordering::Release), block_epoch);
        }
        assert_eq!(self.execute_height.fetch_add(1, Ordering::Release), block_number - 1);
        self.execute_block_barrier
            .notify(
                (epoch, block_number),
                ExecuteBlockContext {
                    parent_header: block.header.clone(),
                    prev_start_execute_time: start_time,
                    epoch,
                },
            )
            .unwrap();

        let execution_outcome = self.calculate_roots(&mut block, execution_output);

        // Merkling the state trie
        self.merklize_barrier.wait(block_number - 1).await.unwrap();
        let start_time = Instant::now();
        let (state_root, trie_updates) = self.storage.state_root(&hashed_state).unwrap();
        let write_start = Instant::now();
        self.cache.write_trie_updates(&trie_updates, block_number);
        self.metrics.cache_trie_state.record(write_start.elapsed());
        let elapsed = start_time.elapsed();
        self.metrics.merklize_duration.record(elapsed);
        self.merklize_barrier.notify(block_number, ()).unwrap();
        info!(target: "PipeExecService.process",
            block_number=?block_number,
            block_id=?block_id,
            state_root=?state_root,
            elapsed=?elapsed,
            "state trie merklized"
        );
        block.header.state_root = state_root;
        block.header.difficulty = randomness;

        // Seal the block
        let parent_hash = self.seal_barrier.wait(block_number - 1).await.unwrap();
        let start_time = Instant::now();
        block.header.parent_hash = parent_hash;
        let sealed_block = block.seal_slow();
        let block_hash = sealed_block.hash();
        self.metrics.seal_duration.record(start_time.elapsed());
        self.seal_barrier.notify(block_number, block_hash).unwrap();
        debug!(target: "PipeExecService.process",
            block_number=?block_number,
            block_id=?block_id,
            block_hash=?block_hash,
            header=?sealed_block.header(),
            "block sealed"
        );

        let gas_used = sealed_block.gas_used;
        let executed_block = ExecutedBlockWithTrieUpdates::new(
            Arc::new(RecoveredBlock::new_sealed(sealed_block, senders)),
            Arc::new(execution_outcome),
            Arc::new(hashed_state),
            ExecutedTrieUpdates::empty(),
            Arc::new(trie_updates),
        );
        let execution_result =
            ExecutionResult { block_id, block_number, block_hash, txs_info, gravity_events };

        self.verify_executed_block_hash(execution_result).await.unwrap();
        self.make_canonical(&block_id, executed_block).await;
        self.metrics.total_gas_used.increment(gas_used);
        self.metrics.end_process_block_number.set(block_number as f64);
    }

    /// Push executed block hash to Coordinator and wait for verification result from Coordinator.
    /// Returns `None` if the channel has been closed.
    async fn verify_executed_block_hash(&self, execution_result: ExecutionResult) -> Option<()> {
        let start_time = Instant::now();
        let block_id = execution_result.block_id;
        let block_number = execution_result.block_number;
        let executed_block_hash = execution_result.block_hash;
        self.execution_result_tx.send(execution_result).ok()?;
        let block_hash = self.verified_block_hash_rx.wait(block_id).await?;
        if let Some(block_hash) = block_hash {
            assert_eq!(executed_block_hash, block_hash);
        }
        let elapsed = start_time.elapsed();
        self.metrics.verify_duration.record(elapsed);
        info!(target: "PipeExecService.process",
            block_number=?block_number,
            block_id=?block_id,
            block_hash=?executed_block_hash,
            elapsed=?elapsed,
            "block verified"
        );
        Some(())
    }

    fn create_block_for_executor(
        &self,
        ordered_block: OrderedBlock,
        base_fee: u64,
        state: &Storage::StateView,
        mut validator_txns: Vec<TransactionSigned>,
    ) -> (RecoveredBlock<Block>, Vec<TxInfo>) {
        assert_eq!(ordered_block.transactions.len(), ordered_block.senders.len());
        let mut block = Block {
            header: Header {
                beneficiary: ordered_block.coinbase,
                timestamp: ordered_block.timestamp_us / 1_000_000, // convert to seconds
                mix_hash: ordered_block.prev_randao,
                base_fee_per_gas: Some(base_fee),
                number: ordered_block.number,
                gas_limit: get_gravity_config().pipe_block_gas_limit,
                ommers_hash: EMPTY_OMMER_ROOT_HASH,
                nonce: BEACON_NONCE.into(),
                ..Default::default()
            },
            body: BlockBody::default(),
        };
        debug!(target: "create_block_for_executor",
            header=?block.header,
            "created block"
        );

        if self.chain_spec.is_shanghai_active_at_timestamp(block.timestamp) {
            if ordered_block.withdrawals.is_empty() {
                block.header.withdrawals_root = Some(EMPTY_WITHDRAWALS);
                block.body.withdrawals = Some(Withdrawals::default());
            } else {
                block.header.withdrawals_root =
                    Some(proofs::calculate_withdrawals_root(&ordered_block.withdrawals));
                block.body.withdrawals = Some(ordered_block.withdrawals);
            }
        }

        // only determine cancun fields when active
        if self.chain_spec.is_cancun_active_at_timestamp(block.timestamp) {
            // FIXME: Is it OK to use the parent's block id as `parent_beacon_block_root` before
            // execution?
            block.header.parent_beacon_block_root = Some(ordered_block.parent_id);

            // TODO(nekomoto): fill `excess_blob_gas` and `blob_gas_used` fields
            block.header.excess_blob_gas = Some(0);
            block.header.blob_gas_used = Some(0);
        }

        // Discard the invalid txs
        let start_time = Instant::now();
        let (txs, senders, txs_info) = self.filter_invalid_txs(
            state,
            ordered_block.transactions,
            ordered_block.senders,
            base_fee,
            block.gas_limit,
        );
        self.metrics.filter_transaction_duration.record(start_time.elapsed());
        let (txs, senders) = if !validator_txns.is_empty() {
            let mut address = vec![SYSTEM_CALLER; validator_txns.len()];
            address.extend(senders);
            validator_txns.extend(txs);
            (validator_txns, address)
        } else {
            (txs, senders)
        };

        block.body.transactions = txs;
        (RecoveredBlock::new_unhashed(block, senders), txs_info)
    }

    /// Execute all system transactions (metadata, DKG, JWK) sequentially
    ///
    /// This function encapsulates the execution of all system-level transactions
    /// that must be processed before the parallel user transaction execution.
    ///
    /// Returns `SystemTxnExecutionOutcome::EpochChanged` if a new epoch was triggered,
    /// otherwise returns `SystemTxnExecutionOutcome::Continue` with the results.
    fn execute_system_transactions(
        evm_config: &EthEvmConfig,
        chain_spec: &ChainSpec,
        state: &Arc<Storage::StateView>,
        evm_env: reth_evm::EvmEnv,
        ordered_block: &OrderedBlock,
        base_fee: u64,
        epoch: u64,
        block_id: B256,
        parent_id: B256,
        block_number: u64,
    ) -> SystemTxnExecutionOutcome {
        let mut inner_state =
            State::builder().with_database_ref(state).with_bundle_update().build();
        let mut evm = evm_config.evm_with_env(&mut inner_state, evm_env);

        // Create shared state for precompile - keep a reference so we can extract changes later
        let state_for_precompile = {
            let report_db_metrics = get_gravity_config().report_db_metrics;
            let parallel_state = ParallelState::new(state.clone(), true, report_db_metrics);
            Arc::new(parking_lot::Mutex::new(parallel_state))
        };
        let state_for_precompile_ref = state_for_precompile.clone();
        let precompile = create_mint_token_precompile(state_for_precompile);
        evm.precompiles_mut()
            .apply_precompile(&NATIVE_MINT_PRECOMPILE_ADDR, move |_| Some(precompile));

        // Get system caller nonce and gas price for constructing all system transactions
        let system_call_account =
            evm.db_mut().basic(SYSTEM_CALLER).unwrap().expect("SYSTEM_CALLER not exists");
        let gas_price = evm.block().basefee as u128;
        let mut current_nonce = system_call_account.nonce;
        // Construct and execute metadata transaction using unified entry point
        let metadata_txn = construct_metadata_txn(
            current_nonce,
            gas_price,
            ordered_block.timestamp_us,
            ordered_block.proposer_index,
        );
        current_nonce = metadata_txn.nonce() + 1;

        let (metadata_txn_result, metadata_state_changes) =
            transact_system_txn(&mut evm, metadata_txn);

        // Commit metadata state changes immediately so subsequent txns see nonce update
        evm.db_mut().commit(metadata_state_changes.clone());

        // Accumulate state changes for returning to executor
        let mut accumulated_state_changes = metadata_state_changes;

        // Check for epoch change
        if let Some((new_epoch, validators)) = metadata_txn_result.emit_new_epoch() {
            drop(evm);
            assert_eq!(new_epoch, epoch + 1);
            info!(target: "execute_ordered_block",
                id=?block_id,
                parent_id=?parent_id,
                number=?block_number,
                new_epoch=?new_epoch,
                "emit new epoch, discard the block"
            );
            inner_state.merge_transitions(BundleRetention::Reverts);
            return SystemTxnExecutionOutcome::EpochChanged(
                metadata_txn_result.into_executed_ordered_block_result(
                    chain_spec,
                    ordered_block,
                    base_fee,
                    inner_state.take_bundle(),
                    validators,
                ),
            );
        }

        debug!(target: "execute_ordered_block",
            metadata_txn_result=?metadata_txn_result,
            "metadata transaction result"
        );

        // Execute validator transactions (DKG and JWK) one by one
        // DKG transactions are executed first since they may trigger epoch changes
        let mut validator_txn_results: Vec<SystemTxnResult> = Vec::new();

        // Sort extra_data: DKG first, then JWK
        let mut sorted_extra_data: Vec<_> = ordered_block.extra_data.iter().collect();
        sorted_extra_data.sort_by_key(|data| match data {
            ExtraDataType::DKG(_) => 0, // DKG comes first
            ExtraDataType::JWK(_) => 1, // JWK comes second
        });

        for (index, extra_data) in sorted_extra_data.iter().enumerate() {
            let is_dkg = matches!(extra_data, ExtraDataType::DKG(_));
            // Try to construct validator transaction, skip if failed
            let txn =
                match construct_validator_txn_from_extra_data(extra_data, current_nonce, gas_price)
                {
                    Ok(txn) => txn,
                    Err(e) => {
                        error!(target: "execute_ordered_block",
                            index=?index,
                            is_dkg=?is_dkg,
                            block_number=?block_number,
                            error=?e,
                            "Failed to construct validator transaction, skipping"
                        );
                        continue;
                    }
                };
            current_nonce += 1;

            debug!(target: "execute_ordered_block",
                index=?index,
                nonce=?current_nonce,
                is_dkg=?is_dkg,
                block_number=?block_number,
                "executing validator transaction one by one"
            );

            let (validator_result, validator_state_changes) = transact_system_txn(&mut evm, txn);

            // Commit state changes immediately so subsequent txns see nonce update
            evm.db_mut().commit(validator_state_changes.clone());

            // Merge state changes into accumulated changes
            for (addr, account) in validator_state_changes {
                accumulated_state_changes.insert(addr, account);
            }

            // DKG transactions may trigger epoch change
            if is_dkg {
                if let Some((new_epoch, validators)) = validator_result.emit_new_epoch() {
                    drop(evm);
                    assert_eq!(new_epoch, epoch + 1);
                    info!(target: "execute_ordered_block",
                        id=?block_id,
                        parent_id=?parent_id,
                        number=?block_number,
                        new_epoch=?new_epoch,
                        "DKG triggered new epoch, discard the block"
                    );
                    inner_state.merge_transitions(BundleRetention::Reverts);
                    return SystemTxnExecutionOutcome::EpochChanged(
                        validator_result.into_executed_ordered_block_result(
                            chain_spec,
                            ordered_block,
                            base_fee,
                            inner_state.take_bundle(),
                            validators,
                        ),
                    );
                }
            }

            info!(target: "execute_ordered_block",
                index=?index,
                is_dkg=?is_dkg,
                gas_used=?validator_result.result.gas_used(),
                block_number=?block_number,
                "validator transaction executed successfully"
            );

            validator_txn_results.push(validator_result);
        }

        drop(evm);

        // Extract changes from precompile state and merge into accumulated_state_changes
        {
            let mut precompile_state = state_for_precompile_ref.lock();
            precompile_state.merge_transitions(BundleRetention::Reverts);
            let precompile_bundle = precompile_state.take_bundle();

            // Convert BundleState to EvmState and merge
            for (address, account) in precompile_bundle.state {
                if let Some(info) = account.info {
                    use revm::state::{Account, AccountStatus, EvmStorageSlot};
                    accumulated_state_changes.insert(
                        address,
                        Account {
                            info,
                            storage: account
                                .storage
                                .into_iter()
                                .map(|(k, v)| (k, EvmStorageSlot::new(v.present_value, 0)))
                                .collect(),
                            status: AccountStatus::Touched,
                            transaction_id: 0,
                        },
                    );
                }
            }
        }

        SystemTxnExecutionOutcome::Continue {
            metadata_result: metadata_txn_result,
            accumulated_state_changes,
            validator_results: validator_txn_results,
        }
    }

    /// Extract gravity events from execution receipts
    /// Returns gravity_events containing DKG events and ObservedJWKsUpdated from DataRecorded
    /// events TODO(gravity): Currently, it executes the entire block and then parses all logs
    /// from the whole block. Theoretically, it could only parse metadata and validator
    /// transactions.
    fn extract_gravity_events_from_receipts(
        &self,
        receipts: &[Receipt],
        block_number: u64,
        epoch: u64,
    ) -> Vec<GravityEvent> {
        use gravity_api_types::on_chain_config::jwks::ProviderJWKs;
        use std::collections::HashMap;

        let mut gravity_events = vec![];
        // Map from (sourceType, sourceId) to latest nonce
        let mut data_records: HashMap<(u32, alloy_primitives::U256), u128> = HashMap::new();

        for receipt in receipts {
            debug!(target: "execute_ordered_block",
                number=?block_number,
                logs_len=?receipt.logs.len(),
                "extract gravity events from receipt"
            );
            for log in &receipt.logs {
                // Parse DataRecorded events from NativeOracle
                if let Ok(event) = DataRecorded::decode_log(&log) {
                    info!(target: "execute_ordered_block",
                        number=?block_number,
                        source_type=?event.sourceType,
                        source_id=?event.sourceId,
                        nonce=?event.nonce,
                        "data recorded event"
                    );
                    // Keep only the latest nonce for each (sourceType, sourceId)
                    let key = (event.sourceType, event.sourceId);
                    data_records
                        .entry(key)
                        .and_modify(|existing_nonce| {
                            if event.nonce > *existing_nonce {
                                *existing_nonce = event.nonce;
                            }
                        })
                        .or_insert(event.nonce);
                }

                // Parse DKG events (unchanged)
                if let Ok(event) = DKGStartEvent::decode_log(&log) {
                    info!(target: "execute_ordered_block",
                        number=?block_number,
                        dealer_epoch=?event.dealerEpoch,
                        "dkg start"
                    );
                    gravity_events.push(GravityEvent::DKG(convert_dkg_start_event_to_api(&event)));
                }
            }
        }

        // Convert collected DataRecorded events to ProviderJWKs
        if !data_records.is_empty() {
            let api_jwks: Vec<ProviderJWKs> = data_records
                .into_iter()
                .map(|((source_type, source_id), nonce)| {
                    // issuer format: "gravity://sourceType/sourceId"
                    let issuer = format!("gravity://{}/{}", source_type, source_id);
                    ProviderJWKs {
                        issuer: issuer.into_bytes(),
                        version: nonce as u64, // nonce as version
                        jwks: vec![],          // return empty jwks
                    }
                })
                .collect();

            info!(target: "execute_ordered_block",
                number=?block_number,
                epoch=?epoch,
                provider_count=?api_jwks.len(),
                "constructed ProviderJWKs from DataRecorded events"
            );

            gravity_events.push(GravityEvent::ObservedJWKsUpdated(epoch, api_jwks));
        }

        gravity_events
    }

    fn execute_ordered_block(
        &self,
        ordered_block: OrderedBlock,
        parent_header: &Header,
    ) -> ExecuteOrderedBlockResult {
        let block_id = ordered_block.id;
        let parent_id = ordered_block.parent_id;
        let block_number = ordered_block.number;
        assert_eq!(block_number, parent_header.number + 1);
        let epoch = ordered_block.epoch;

        let state = Arc::new(self.storage.get_state_view().unwrap());

        let evm_env = self
            .evm_config
            .next_evm_env(
                parent_header,
                &NextBlockEnvAttributes {
                    timestamp: ordered_block.timestamp_us / 1_000_000, // convert to seconds
                    suggested_fee_recipient: ordered_block.coinbase,
                    prev_randao: ordered_block.prev_randao,
                    gas_limit: get_gravity_config().pipe_block_gas_limit,
                    parent_beacon_block_root: Some(ordered_block.parent_id),
                    withdrawals: Some(ordered_block.withdrawals.clone()),
                },
            )
            .unwrap();
        debug!(target: "execute_ordered_block",
            evm_env=?evm_env,
            block_number=?block_number,
        );
        let base_fee = evm_env.block_env.basefee;
        //             let mut evm = self.evm_config.evm_with_env(&mut state, evm_env);
        //  evm apply precompile
        // for xx in xx {  evm.execute_transaction(tx) }
        // cases

        // Execute system transactions (metadata, DKG, JWK) sequentially
        let (metadata_txn_result, accumulated_state_changes, validator_txn_results) =
            match Self::execute_system_transactions(
                &self.evm_config,
                &self.chain_spec,
                &state,
                evm_env,
                &ordered_block,
                base_fee,
                epoch,
                block_id,
                parent_id,
                block_number,
            ) {
                SystemTxnExecutionOutcome::EpochChanged(result) => return result,
                SystemTxnExecutionOutcome::Continue {
                    metadata_result,
                    accumulated_state_changes,
                    validator_results,
                } => (metadata_result, accumulated_state_changes, validator_results),
            };

        // No longer pass validator_txns to create_block_for_executor since they are executed
        // separately
        let (block, txs_info) =
            self.create_block_for_executor(ordered_block, base_fee, &state, vec![]);

        info!(target: "execute_ordered_block",
            id=?block_id,
            parent_id=?parent_id,
            number=?block_number,
            num_txs=?block.transaction_count(),
            validator_txn_count=?validator_txn_results.len(),
            "ready to execute block"
        );

        let mut executor = self.evm_config.parallel_executor(state);
        // Apply all pre-executed transaction state changes (metadata + validator txns) to executor
        // state
        executor.commit_changes(accumulated_state_changes);
        let outcome = executor.execute(&block).unwrap_or_else(|err| {
            serde_json::to_writer_pretty(
                std::io::BufWriter::new(std::fs::File::create(format!("{block_id}.json")).unwrap()),
                &block,
            )
            .unwrap();
            panic!("failed to execute block {block_id:?}: {err:?}")
        });
        info!(target: "execute_ordered_block",
            id=?block_id,
            parent_id=?parent_id,
            number=?block_number,
            receipts_len=?outcome.receipts.len(),
            "executed block done"
        );

        let (mut block, senders) = block.split();
        block.header.gas_used = outcome.gas_used;
        let mut result = ExecuteOrderedBlockResult {
            block,
            senders,
            execution_output: outcome,
            txs_info,
            gravity_events: vec![],
            epoch,
        };
        metadata_txn_result.insert_to_executed_ordered_block_result(&mut result, 0);
        // Insert validator transaction results one by one after the metadata transaction
        // Position 1 is right after the metadata transaction at position 0
        for (index, validator_result) in validator_txn_results.into_iter().enumerate() {
            validator_result.insert_to_executed_ordered_block_result(&mut result, 1 + index);
        }
        debug!(target: "execute_ordered_block",
            number=?result.block.number,
            receipts_len=?result.execution_output.receipts.len(),
            "insert metadata and validator transaction results to executed ordered block result"
        );
        let gravity_events = self.extract_gravity_events_from_receipts(
            &result.execution_output.receipts,
            result.block.number,
            epoch,
        );

        result.gravity_events.extend(gravity_events);
        result
    }

    /// Only used for testing.
    fn execute_history_block(&self, block: RecoveredBlock<Block>) -> ExecuteOrderedBlockResult {
        let state = self.storage.get_state_view().unwrap();
        let mut executor = self.evm_config.parallel_executor(state);
        let outcome = executor.execute(&block).unwrap_or_else(|err| {
            serde_json::to_writer(
                std::io::BufWriter::new(
                    std::fs::File::create(format!("{}.json", block.number)).unwrap(),
                ),
                &block,
            )
            .unwrap();
            panic!("failed to execute block {:?}: {:?}", block.number, err)
        });
        let (block, senders) = block.split();
        ExecuteOrderedBlockResult {
            block,
            senders,
            execution_output: outcome,
            txs_info: vec![],
            gravity_events: vec![],
            epoch: 0,
        }
    }

    /// Calculate the receipts root, logs bloom, and transactions root, etc. and fill them into the
    /// block header.
    fn calculate_roots(
        &self,
        block: &mut Block,
        execution_output: BlockExecutionOutput<Receipt>,
    ) -> ExecutionOutcome {
        // only determine cancun fields when active
        if self.chain_spec.is_prague_active_at_timestamp(block.timestamp) {
            block.header.requests_hash = Some(execution_output.requests.requests_hash());
        }

        let execution_outcome = ExecutionOutcome::new(
            execution_output.state,
            vec![execution_output.result.receipts],
            block.number,
            vec![execution_output.result.requests],
        );

        // Fill the block header with the calculated values
        block.header.transactions_root =
            proofs::calculate_transaction_root(&block.body.transactions);
        if self.chain_spec.is_byzantium_active_at_block(block.number()) {
            block.header.receipts_root =
                execution_outcome.ethereum_receipts_root(block.number).unwrap();
            block.header.logs_bloom = execution_outcome.block_logs_bloom(block.number).unwrap();
        }

        execution_outcome
    }

    async fn make_canonical(&self, block_id: &B256, executed_block: ExecutedBlockWithTrieUpdates) {
        let block_number = executed_block.recovered_block.number();
        let block_hash = executed_block.recovered_block.hash();
        let prev_finish_commit_time =
            self.make_canonical_barrier.wait(block_number - 1).await.unwrap();
        let start_time = Instant::now();
        let (tx, rx) = oneshot::channel();
        self.event_tx
            .send(PipeExecLayerEvent::MakeCanonical(MakeCanonicalEvent { executed_block, tx }))
            .unwrap();
        rx.await.unwrap();
        self.storage.update_canonical(block_number, block_hash);
        let elapsed = start_time.elapsed();
        info!(target: "PipeExecService.process",
            block_number=?block_number,
            block_id=?block_id,
            block_hash=?block_hash,
            elapsed=?elapsed,
            "block made canonical"
        );
        let finish_commit_time = Instant::now();
        self.metrics.make_canonical_duration.record(elapsed);
        self.metrics.finish_commit_time_diff.record(finish_commit_time - prev_finish_commit_time);
        self.make_canonical_barrier.notify(block_number, finish_commit_time).unwrap();
    }

    fn init_storage(&self, execution_args: ExecutionArgs) {
        execution_args.block_number_to_block_id.into_iter().for_each(|(block_number, block_id)| {
            self.storage.insert_block_id(block_number, block_id);
        });
    }

    /// Return the filtered valid transactions with sender without changing the relative order of
    /// the transactions.
    fn filter_invalid_txs(
        &self,
        db: &Storage::StateView,
        txs: Vec<TransactionSigned>,
        senders: Vec<Address>,
        base_fee_per_gas: u64,
        gas_limit: u64,
    ) -> (Vec<TransactionSigned>, Vec<Address>, Vec<TxInfo>) {
        let invalid_idxs = filter_invalid_txs(db, &txs, &senders, base_fee_per_gas, gas_limit);
        if invalid_idxs.is_empty() {
            let mut txs_info = Vec::with_capacity(txs.len());
            for (tx, sender) in txs.iter().zip(senders.iter()) {
                txs_info.push(TxInfo {
                    tx_hash: *tx.hash(),
                    sender: *sender,
                    nonce: tx.nonce(),
                    is_discarded: false,
                });
            }
            (txs, senders, txs_info)
        } else {
            let _ = self
                .discard_txs_tx
                .send(invalid_idxs.iter().map(|&idx| txs[idx].hash()).copied().collect::<Vec<_>>());

            let mut filtered_txs = Vec::with_capacity(txs.len() - invalid_idxs.len());
            let mut filtered_senders = Vec::with_capacity(filtered_txs.capacity());
            let mut txs_info = Vec::with_capacity(txs.len());
            for (i, (tx, sender)) in txs.into_iter().zip(senders.into_iter()).enumerate() {
                if invalid_idxs.contains(&i) {
                    txs_info.push(TxInfo {
                        tx_hash: *tx.hash(),
                        sender,
                        nonce: tx.nonce(),
                        is_discarded: true,
                    });
                    continue;
                }

                txs_info.push(TxInfo {
                    tx_hash: *tx.hash(),
                    sender,
                    nonce: tx.nonce(),
                    is_discarded: false,
                });
                filtered_txs.push(tx);
                filtered_senders.push(sender);
            }
            (filtered_txs, filtered_senders, txs_info)
        }
    }
}

/// Return the invalid transaction indexes.
fn filter_invalid_txs<DB: ParallelDatabase>(
    db: DB,
    txs: &[TransactionSigned],
    senders: &[Address],
    base_fee_per_gas: u64,
    gas_limit: u64,
) -> HashSet<usize> {
    let mut gas_limit_exceeded_tx_idx = txs.len();
    let mut tx_gas_limit_sum = 0;
    for (idx, tx) in txs.iter().enumerate() {
        let tx_gas_limit = tx.gas_limit();
        if tx_gas_limit_sum + tx_gas_limit > gas_limit {
            warn!(target: "filter_invalid_txs",
                tx_hash=?txs[idx].hash(),
                sender=?senders[idx],
                block_gas_limit=?gas_limit,
                "gas limit exceeded, truncated to {}",
                idx,
            );
            gas_limit_exceeded_tx_idx = idx;
            break;
        } else {
            tx_gas_limit_sum += tx_gas_limit;
        }
    }

    let mut sender_idx: HashMap<&Address, Vec<usize>> = HashMap::default();
    for (i, sender) in senders[..gas_limit_exceeded_tx_idx].iter().enumerate() {
        sender_idx.entry(sender).or_default().push(i);
    }

    let is_tx_valid = |tx: &TransactionSigned, sender: &Address, account: &mut AccountInfo| {
        if account.nonce != tx.nonce() {
            warn!(target: "filter_invalid_txs",
                tx_hash=?tx.hash(),
                sender=?sender,
                nonce=?tx.nonce(),
                account_nonce=?account.nonce,
                "nonce mismatch"
            );
            return false;
        }
        let gas_spent = U256::from(tx.effective_gas_price(Some(base_fee_per_gas)))
            .saturating_mul(U256::from(tx.gas_limit()));
        let total_spent = gas_spent.saturating_add(tx.value());
        if account.balance < total_spent {
            warn!(target: "filter_invalid_txs",
                tx_hash=?tx.hash(),
                sender=?sender,
                balance=?account.balance,
                gas_spent=?gas_spent,
                transfer_value=?tx.value(),
                "insufficient balance"
            );
            return false;
        }
        account.balance -= total_spent;
        account.nonce += 1;
        true
    };

    let mut invalid_tx_idxs = sender_idx
        .into_par_iter()
        .flat_map(|(sender, idxs)| {
            if let Some(mut account) = db.basic_ref(*sender).unwrap() {
                idxs.into_iter()
                    .filter(|&idx| !is_tx_valid(&txs[idx], sender, &mut account))
                    .collect()
            } else {
                // Sender does not exist in the state trie, balance is 0
                warn!(target: "filter_invalid_txs",
                    tx_hash=?txs[idxs[0]].hash(),
                    sender=?sender,
                    "insufficient balance"
                );
                idxs
            }
        })
        .collect::<HashSet<_>>();
    invalid_tx_idxs.extend(gas_limit_exceeded_tx_idx..txs.len());
    invalid_tx_idxs
}

/// Called by Coordinator
#[derive(Debug)]
pub struct PipeExecLayerApi<Storage, EthApi> {
    ordered_block_tx: UnboundedSender<ReceivedBlock>,
    execution_result_rx: Mutex<UnboundedReceiver<ExecutionResult>>,
    verified_block_hash_tx: Arc<Channel<B256 /* block id */, Option<B256> /* block hash */>>,
    event_tx: std::sync::mpsc::Sender<PipeExecLayerEvent<EthPrimitives>>,
    storage: Arc<Storage>,
    onchain_config_fetcher: OnchainConfigFetcher<EthApi>,
}

impl<Storage, EthApi> ConfigStorage for PipeExecLayerApi<Storage, EthApi>
where
    Storage: Sync + Send + 'static,
    EthApi: EthCall,
    EthApi::NetworkTypes: RpcTypes<TransactionRequest = TransactionRequest>,
{
    fn fetch_config_bytes(
        &self,
        config_name: OnChainConfig,
        block_id: BlockNumber,
    ) -> Option<OnChainConfigResType> {
        self.onchain_config_fetcher.fetch_config_bytes(
            config_name,
            match block_id {
                BlockNumber::Number(number) => number.into(),
                BlockNumber::Latest => alloy_eips::BlockId::Number(BlockNumberOrTag::Latest),
            },
        )
    }
}

impl<Storage: GravityStorage, EthApi> PipeExecLayerApi<Storage, EthApi> {
    /// Push ordered block to EL for execution.
    /// Returns `None` if the channel has been closed.
    pub fn push_ordered_block(&self, block: OrderedBlock) -> Option<()> {
        self.ordered_block_tx.send(ReceivedBlock::OrderedBlock(block)).ok()
    }

    /// Only used for testing.
    pub fn push_history_block(&self, block: RecoveredBlock<Block>) -> Option<()> {
        self.ordered_block_tx.send(ReceivedBlock::HistoryBlock(block.into())).ok()
    }

    /// Pull executed block hash from EL for verification.
    /// Returns `None` if the channel has been closed.
    pub async fn pull_executed_block_hash(&self) -> Option<ExecutionResult> {
        self.execution_result_rx.lock().await.recv().await
    }

    /// Push verified block hash to EL for commit.
    /// The caller can optionally pass in a verified block hash, which is solely used for the EL
    /// defensive check to ensure the consistency of the block hash before and after verification.
    /// Returns `None` if the channel has been closed.
    pub fn commit_executed_block_hash(
        &self,
        block_id: B256,
        block_hash: Option<B256>,
    ) -> Option<()> {
        self.verified_block_hash_tx.notify(block_id, block_hash)
    }

    /// Wait for the block with the given block number to be persisted in the storage.
    /// Returns `None` if the channel has been closed.
    pub async fn wait_for_block_persistence(&self, block_number: u64) -> Option<()> {
        let (tx, rx) = oneshot::channel();
        self.event_tx
            .send(PipeExecLayerEvent::WaitForPersistence(WaitForPersistenceEvent {
                block_number,
                tx,
            }))
            .ok()?;
        rx.await.ok()
    }

    /// Get the block id by block number.
    pub fn get_block_id(&self, block_number: u64) -> Option<B256> {
        self.storage.get_block_id(block_number)
    }
}

impl<Storage, EthApi> Drop for PipeExecLayerApi<Storage, EthApi> {
    fn drop(&mut self) {
        self.verified_block_hash_tx.close();
    }
}

/// Create a new `PipeExecLayerApi` instance and launch a `PipeExecService`.
pub fn new_pipe_exec_layer_api<Storage, EthApi>(
    chain_spec: Arc<ChainSpec>,
    storage: Storage,
    latest_block_header: Header,
    latest_block_hash: B256,
    execution_args_rx: oneshot::Receiver<ExecutionArgs>,
    eth_api: EthApi,
) -> PipeExecLayerApi<Storage, EthApi>
where
    Storage: GravityStorage,
    EthApi: EthCall,
    EthApi::NetworkTypes: RpcTypes<TransactionRequest = TransactionRequest>,
{
    let (ordered_block_tx, ordered_block_rx) = tokio::sync::mpsc::unbounded_channel();
    let (execution_result_tx, execution_result_rx) = tokio::sync::mpsc::unbounded_channel();
    let verified_block_hash_ch = Arc::new(Channel::new());
    let (event_tx, event_rx) = std::sync::mpsc::channel();
    let (discard_txs_tx, discard_txs_rx) = tokio::sync::mpsc::unbounded_channel();

    let storage = Arc::new(storage);
    let onchain_config_fetcher = OnchainConfigFetcher::new(eth_api);

    let latest_block_number = latest_block_header.number;
    let epoch = onchain_config_fetcher
        .fetch_epoch(latest_block_number.into())
        .expect("Failed to fetch epoch");
    info!(target: "PipeExecService.new_pipe_exec_layer_api",
        latest_block_number=?latest_block_number,
        latest_block_hash=?latest_block_hash,
        epoch=?epoch,
        "new pipe exec layer api"
    );

    let start_time = Instant::now();
    let service = PipeExecService {
        core: Arc::new(Core {
            execution_result_tx,
            verified_block_hash_rx: verified_block_hash_ch.clone(),
            storage: storage.clone(),
            evm_config: EthEvmConfig::new(chain_spec.clone()),
            chain_spec,
            event_tx: event_tx.clone(),
            execute_block_barrier: Channel::new_with_states([(
                (epoch, latest_block_number),
                ExecuteBlockContext {
                    parent_header: latest_block_header,
                    prev_start_execute_time: start_time,
                    epoch,
                },
            )]),
            merklize_barrier: Channel::new_with_states([(latest_block_number, ())]),
            seal_barrier: Channel::new_with_states([(latest_block_number, latest_block_hash)]),
            make_canonical_barrier: Channel::new_with_states([(latest_block_number, start_time)]),
            discard_txs_tx,
            cache: PERSIST_BLOCK_CACHE.clone(),
            epoch: AtomicU64::new(epoch),
            execute_height: AtomicU64::new(latest_block_number),
            metrics: PipeExecLayerMetrics::default(),
        }),
        ordered_block_rx,
        execution_args_rx,
    };
    tokio::spawn(service.run());

    PIPE_EXEC_LAYER_EVENT_BUS.get_or_init(|| {
        Box::new(PipeExecLayerEventBus {
            event_rx: std::sync::Mutex::new(Some(event_rx)),
            discard_txs: tokio::sync::Mutex::new(Some(discard_txs_rx)),
        })
    });

    PipeExecLayerApi {
        ordered_block_tx,
        execution_result_rx: Mutex::new(execution_result_rx),
        verified_block_hash_tx: verified_block_hash_ch,
        event_tx,
        storage,
        onchain_config_fetcher,
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use alloy_consensus::TxLegacy;
    use alloy_primitives::{Address, Signature, U256};
    use reth_ethereum_primitives::{Transaction, TransactionSigned};
    use reth_revm::state::{AccountInfo, Bytecode};
    use revm::{
        primitives::{StorageKey, StorageValue},
        DatabaseRef,
    };
    use std::collections::HashMap;

    // Mock database for testing
    #[derive(Debug, Default)]
    struct MockDatabase {
        accounts: HashMap<Address, AccountInfo>,
    }

    impl MockDatabase {
        fn new() -> Self {
            Self::default()
        }

        fn insert_account(&mut self, address: Address, account: AccountInfo) {
            self.accounts.insert(address, account);
        }
    }

    impl DatabaseRef for MockDatabase {
        type Error = std::convert::Infallible;

        fn basic_ref(&self, address: Address) -> Result<Option<AccountInfo>, Self::Error> {
            Ok(self.accounts.get(&address).cloned())
        }

        fn code_by_hash_ref(&self, _code_hash: B256) -> Result<Bytecode, Self::Error> {
            unreachable!()
        }

        fn storage_ref(
            &self,
            _address: Address,
            _index: StorageKey,
        ) -> Result<StorageValue, Self::Error> {
            unreachable!()
        }

        fn block_hash_ref(&self, _number: u64) -> Result<B256, Self::Error> {
            unreachable!()
        }
    }

    fn create_test_transaction(nonce: u64, gas_limit: u64, gas_price: u128) -> TransactionSigned {
        TransactionSigned::new_unhashed(
            Transaction::Legacy(TxLegacy { nonce, gas_price, gas_limit, ..Default::default() }),
            Signature::test_signature(),
        )
    }

    fn create_test_transaction_with_value(
        nonce: u64,
        gas_limit: u64,
        gas_price: u128,
        value: U256,
    ) -> TransactionSigned {
        TransactionSigned::new_unhashed(
            Transaction::Legacy(TxLegacy {
                nonce,
                gas_price,
                gas_limit,
                value,
                ..Default::default()
            }),
            Signature::test_signature(),
        )
    }

    #[test]
    fn test_filter_invalid_txs_empty_input() {
        let db = MockDatabase::new();
        let txs = vec![];
        let senders = vec![];
        let base_fee_per_gas = 20_000_000_000u64; // 20 gwei
        let gas_limit = 30_000_000u64; // 30M gas

        let invalid_idxs = filter_invalid_txs(&db, &txs, &senders, base_fee_per_gas, gas_limit);
        assert!(invalid_idxs.is_empty());
    }

    #[test]
    fn test_filter_invalid_txs_account_not_exists() {
        let db = MockDatabase::new();
        let sender = Address::random();

        // create a transaction, but the account does not exist
        let tx = create_test_transaction(0, 21_000, 25_000_000_000);
        let txs = vec![tx];
        let senders = vec![sender];
        let base_fee_per_gas = 20_000_000_000u64;
        let gas_limit = 30_000_000u64;

        let invalid_idxs = filter_invalid_txs(&db, &txs, &senders, base_fee_per_gas, gas_limit);
        assert_eq!(invalid_idxs.len(), 1);
        assert!(invalid_idxs.contains(&0));
    }

    #[test]
    fn test_filter_invalid_txs_nonce_mismatch() {
        let mut db = MockDatabase::new();
        let sender = Address::random();

        // the account exists, but the nonce does not match
        let account = AccountInfo {
            balance: U256::from(1_000_000_000_000_000_000u64), // 1 ETH
            nonce: 5,                                          //  nonce  5
            code_hash: B256::default(),
            code: None,
        };
        db.insert_account(sender, account);

        // the transaction nonce is 0, but the account nonce is 5
        let tx = create_test_transaction(0, 21_000, 25_000_000_000);
        let txs = vec![tx];
        let senders = vec![sender];
        let base_fee_per_gas = 20_000_000_000u64;
        let gas_limit = 30_000_000u64;

        let invalid_idxs = filter_invalid_txs(&db, &txs, &senders, base_fee_per_gas, gas_limit);
        assert_eq!(invalid_idxs.len(), 1);
        assert!(invalid_idxs.contains(&0));
    }

    #[test]
    fn test_filter_invalid_txs_insufficient_balance() {
        let mut db = MockDatabase::new();
        let sender = Address::random();

        // the account has insufficient balance
        let account = AccountInfo {
            balance: U256::from(1_000_000_000u64), // 1 Gwei
            nonce: 0,
            code_hash: B256::default(),
            code: None,
        };
        db.insert_account(sender, account);

        // fee = gas_price * gas_limit + value = 25_000_000_000 * 21_000 + 0 =
        // 525_000_000_000_000
        let tx1 = create_test_transaction(0, 21_000, 25_000_000_000);
        let tx2 = create_test_transaction_with_value(0, 21_000, 1_000, U256::from(500_000_000u64));
        let tx3 = create_test_transaction_with_value(0, 21_000, 1_000, U256::from(500_000_000u64));
        let txs = vec![tx1, tx2, tx3];
        let senders = vec![sender, sender, sender];
        let base_fee_per_gas = 1_000;
        let gas_limit = 30_000_000u64;

        let invalid_idxs = filter_invalid_txs(&db, &txs, &senders, base_fee_per_gas, gas_limit);
        assert_eq!(invalid_idxs.len(), 2);
        assert!(invalid_idxs.contains(&0));
        assert!(invalid_idxs.contains(&2));
    }

    #[test]
    fn test_filter_invalid_txs_gas_limit_exceeded() {
        let mut db = MockDatabase::new();
        let sender = Address::random();

        // the account has enough balance
        let account = AccountInfo {
            balance: U256::from(1_000_000_000_000_000_000u64), // 1 ETH
            nonce: 0,
            code_hash: B256::default(),
            code: None,
        };
        db.insert_account(sender, account);

        // create multiple transactions, the cumulative gas limit exceeds the block limit
        let tx1 = create_test_transaction(0, 20_000_000, 25_000_000_000); // 20M gas
        let tx2 = create_test_transaction(1, 20_000_000, 25_000_000_000); // 20M gas
        let txs = vec![tx1, tx2];
        let senders = vec![sender, sender];
        let base_fee_per_gas = 20_000_000_000u64;
        let gas_limit = 30_000_000u64; // 30M gas limit

        let invalid_idxs = filter_invalid_txs(&db, &txs, &senders, base_fee_per_gas, gas_limit);
        assert_eq!(invalid_idxs.len(), 1);
        assert!(invalid_idxs.contains(&1));
    }

    #[test]
    fn test_filter_invalid_txs_valid_transactions() {
        let mut db = MockDatabase::new();
        let sender = Address::random();

        // 
        let account = AccountInfo {
            balance: U256::from(1_000_000_000_000_000_000u64), // 1 ETH
            nonce: 0,
            code_hash: B256::default(),
            code: None,
        };
        db.insert_account(sender, account);

        // create valid transactions
        let tx1 = create_test_transaction(0, 21_000, 25_000_000_000);
        let tx2 = create_test_transaction(1, 21_000, 25_000_000_000);
        let txs = vec![tx1, tx2];
        let senders = vec![sender, sender];
        let base_fee_per_gas = 20_000_000_000u64;
        let gas_limit = 30_000_000u64;

        let invalid_idxs = filter_invalid_txs(&db, &txs, &senders, base_fee_per_gas, gas_limit);
        assert!(invalid_idxs.is_empty());
    }

    #[test]
    fn test_filter_invalid_txs_mixed_scenarios() {
        let mut db = MockDatabase::new();
        let sender1 = Address::random();
        let sender2 = Address::random();
        let sender3 = Address::random();

        let account1 = AccountInfo {
            balance: U256::from(1_000_000_000u64), // 1 Gwei
            nonce: 0,
            code_hash: B256::default(),
            code: None,
        };
        db.insert_account(sender1, account1);

        let account2 = AccountInfo {
            balance: U256::from(1_000_000_000u64), // 1 Gwei
            nonce: 5,
            code_hash: B256::default(),
            code: None,
        };
        db.insert_account(sender2, account2);

        let account3 = AccountInfo {
            balance: U256::from(1_000_000_000u64), // 1 Gwei
            nonce: 0,
            code_hash: B256::default(),
            code: None,
        };
        db.insert_account(sender3, account3);

        // create mixed scenarios transactions
        let tx1 = create_test_transaction(0, 21_000, 25); // sender1: valid
        let tx2 = create_test_transaction(0, 21_000, 25); // sender1: nonce does not match
        let tx3 = create_test_transaction(1, 21_000, 25_000_000); // sender1: insufficient balance
        let tx4 = create_test_transaction(5, 21_000, 25); // sender2: valid
        let tx5 = create_test_transaction(2, 21_000, 25); // sender1: nonce does not match
        let tx6 = create_test_transaction(6, 30_000_000, 25); // sender2: gas limit exceeds
        let tx7 = create_test_transaction(0, 21000, 25); // sender3: truncated
        let txs = vec![tx1, tx2, tx3, tx4, tx5, tx6, tx7];
        let senders = vec![sender1, sender1, sender1, sender2, sender2, sender2, sender3];
        let base_fee_per_gas = 20_000_000_000u64;
        let gas_limit = 30_000_000u64;

        let invalid_idxs = filter_invalid_txs(&db, &txs, &senders, base_fee_per_gas, gas_limit);
        assert_eq!(invalid_idxs.len(), 5, "invalid_idxs: {invalid_idxs:?}");
        assert!(invalid_idxs.contains(&1));
        assert!(invalid_idxs.contains(&2));
        assert!(invalid_idxs.contains(&4));
        assert!(invalid_idxs.contains(&5));
        assert!(invalid_idxs.contains(&6));
    }
}
